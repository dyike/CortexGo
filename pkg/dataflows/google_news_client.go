package dataflows

import (
	"encoding/json"
	"encoding/xml"
	"fmt"
	"net/url"
	"os"
	"path/filepath"
	"regexp"
	"strings"
	"time"

	"github.com/PuerkitoBio/goquery"
	"github.com/go-resty/resty/v2"
)

// RSS ç»“æ„ä½“å®šä¹‰
type RSS struct {
	XMLName xml.Name `xml:"rss"`
	Channel Channel  `xml:"channel"`
}

type Channel struct {
	Title       string `xml:"title"`
	Description string `xml:"description"`
	Items       []Item `xml:"item"`
}

type Item struct {
	Title       string    `xml:"title"`
	Link        string    `xml:"link"`
	Description string    `xml:"description"`
	PubDate     string    `xml:"pubDate"`
	Source      Source    `xml:"source"`
	GUID        string    `xml:"guid"`
}

type Source struct {
	URL  string `xml:"url,attr"`
	Text string `xml:",chardata"`
}

// GoogleNewsClient handles Google News operations
type GoogleNewsClient struct {
	client *resty.Client
	cache  *CacheManager
}

// NewGoogleNewsClient creates a new Google News client
func NewGoogleNewsClient(config *Config) *GoogleNewsClient {
	cacheDir := filepath.Join(config.DataCacheDir, "google_news")
	cache := NewCacheManager(cacheDir, 30*time.Minute, config.CacheEnabled) // 30 minute cache for news

	client := resty.New()
	client.SetTimeout(30 * time.Second)
	client.SetHeader("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")

	return &GoogleNewsClient{
		client: client,
		cache:  cache,
	}
}

// EnhancedGoogleNewsParams represents enhanced parameters for Google News search
type EnhancedGoogleNewsParams struct {
	Query      string    `json:"query"`
	Language   string    `json:"language"`   // en, zh-CN, zh-TW, etc.
	Country    string    `json:"country"`    // US, CN, TW, etc.
	StartDate  time.Time `json:"start_date"`
	EndDate    time.Time `json:"end_date"`
	MaxResults int       `json:"max_results"`
	SortBy     string    `json:"sort_by"`    // date, relevance
	Category   string    `json:"category"`   // business, technology, health, etc.
	Site       string    `json:"site"`       // specific site like "reuters.com"
}

// GetGoogleNews performs enhanced Google News search
func (gnc *GoogleNewsClient) GetGoogleNews(params EnhancedGoogleNewsParams, config *Config) ([]*NewsArticle, error) {
	if strings.TrimSpace(params.Query) == "" {
		return nil, fmt.Errorf("search query cannot be empty")
	}

	// Set defaults
	if params.Language == "" {
		params.Language = "en"
	}
	if params.Country == "" {
		params.Country = "US"
	}
	if params.MaxResults <= 0 {
		params.MaxResults = 20
	}
	if params.SortBy == "" {
		params.SortBy = "date"
	}

	// Check cache first
	cacheKey := fmt.Sprintf("%s_%s_%s_%s_%d", params.Query, params.Language, params.Country, params.SortBy, params.MaxResults)
	var cached []*NewsArticle
	if gnc.cache.Get("enhanced_search", "query", cacheKey, &cached) {
		return cached, nil
	}

	// Try multiple search strategies
	var allResults []*NewsArticle

	// Strategy 1: Direct Google News search
	newsResults, err := gnc.searchGoogleNewsDirect(params)
	if err == nil {
		allResults = append(allResults, newsResults...)
	}

	// Strategy 2: Google Search with news filter
	if len(allResults) < params.MaxResults {
		googleResults, err := gnc.searchGoogleWithNewsFilter(params)
		if err == nil {
			allResults = append(allResults, googleResults...)
		}
	}

	// Remove duplicates and limit results
	allResults = gnc.removeDuplicates(allResults)
	if len(allResults) > params.MaxResults {
		allResults = allResults[:params.MaxResults]
	}

	// Cache the result
	gnc.cache.Set("enhanced_search", "query", cacheKey, allResults)

	// Save to file
	filePath := filepath.Join(config.DataDir, "news_data",
		fmt.Sprintf("google_news_enhanced_%s_%s.json",
			strings.ReplaceAll(params.Query, " ", "_"),
			time.Now().Format("2006-01-02")))
	SaveDataToFile(allResults, filePath)

	return allResults, nil
}

// searchGoogleNewsDirect searches Google News directly
func (gnc *GoogleNewsClient) searchGoogleNewsDirect(params EnhancedGoogleNewsParams) ([]*NewsArticle, error) {
	searchURL := gnc.buildGoogleNewsURL(params)

	var result []*NewsArticle
	err := WithRetry(DefaultRetryConfig(), func() error {
		resp, err := gnc.client.R().Get(searchURL)
		if err != nil {
			return fmt.Errorf("failed to fetch Google News: %w", err)
		}

		if resp.StatusCode() != 200 {
			return fmt.Errorf("HTTP error %d when fetching Google News", resp.StatusCode())
		}

		// Parse HTML response
		doc, err := goquery.NewDocumentFromReader(strings.NewReader(resp.String()))
		if err != nil {
			return fmt.Errorf("failed to parse HTML: %w", err)
		}

		result = gnc.parseGoogleNewsHTML(doc, params.Query)
		return nil
	})

	return result, err
}

// searchGoogleWithNewsFilter searches Google with news filter
func (gnc *GoogleNewsClient) searchGoogleWithNewsFilter(params EnhancedGoogleNewsParams) ([]*NewsArticle, error) {
	searchURL := gnc.buildGoogleSearchNewsURL(params)

	var result []*NewsArticle
	err := WithRetry(DefaultRetryConfig(), func() error {
		resp, err := gnc.client.R().Get(searchURL)
		if err != nil {
			return fmt.Errorf("failed to fetch Google Search News: %w", err)
		}

		if resp.StatusCode() != 200 {
			return fmt.Errorf("HTTP error %d when fetching Google Search News", resp.StatusCode())
		}

		// Parse HTML response
		doc, err := goquery.NewDocumentFromReader(strings.NewReader(resp.String()))
		if err != nil {
			return fmt.Errorf("failed to parse HTML: %w", err)
		}

		result = gnc.parseGoogleSearchNewsHTML(doc, params.Query)
		return nil
	})

	return result, err
}

// buildGoogleNewsURL constructs the Google News search URL
func (gnc *GoogleNewsClient) buildGoogleNewsURL(params EnhancedGoogleNewsParams) string {
	baseURL := "https://news.google.com/search"

	query := url.QueryEscape(params.Query)

	// Add category filter
	if params.Category != "" {
		query += url.QueryEscape(fmt.Sprintf(" category:%s", params.Category))
	}

	// Add site filter
	if params.Site != "" {
		query += url.QueryEscape(fmt.Sprintf(" site:%s", params.Site))
	}

	// Add date range if specified
	if !params.StartDate.IsZero() && !params.EndDate.IsZero() {
		dateQuery := fmt.Sprintf(" after:%s before:%s",
			params.StartDate.Format("2006-01-02"),
			params.EndDate.Format("2006-01-02"))
		query += url.QueryEscape(dateQuery)
	}

	return fmt.Sprintf("%s?q=%s&hl=%s&gl=%s&ceid=%s:%s",
		baseURL, query, params.Language, params.Country, params.Country, params.Language)
}

// buildGoogleSearchNewsURL constructs the Google Search with news filter URL
func (gnc *GoogleNewsClient) buildGoogleSearchNewsURL(params EnhancedGoogleNewsParams) string {
	baseURL := "https://www.google.com/search"

	query := url.QueryEscape(params.Query)

	// Add site filter
	if params.Site != "" {
		query += url.QueryEscape(fmt.Sprintf(" site:%s", params.Site))
	}

	values := url.Values{}
	values.Set("q", params.Query)
	values.Set("tbm", "nws") // News search
	values.Set("hl", params.Language)
	values.Set("gl", params.Country)
	values.Set("num", fmt.Sprintf("%d", params.MaxResults))

	// Add sort parameter
	if params.SortBy == "date" {
		values.Set("tbs", "sbd:1") // Sort by date
	}

	return fmt.Sprintf("%s?%s", baseURL, values.Encode())
}

// parseGoogleNewsHTML extracts articles from Google News HTML
func (gnc *GoogleNewsClient) parseGoogleNewsHTML(doc *goquery.Document, query string) []*NewsArticle {
	var articles []*NewsArticle

	// Try multiple selectors for Google News structure
	selectors := []string{
		"article",
		"[data-n-tid]",
		".JtKRv",
		".WwrzSb",
	}

	for _, selector := range selectors {
		doc.Find(selector).Each(func(i int, s *goquery.Selection) {
			article := gnc.extractGoogleNewsArticle(s, query)
			if article != nil {
				articles = append(articles, article)
			}
		})

		if len(articles) > 0 {
			break // Use the first successful selector
		}
	}

	return articles
}

// parseGoogleSearchNewsHTML extracts articles from Google Search News HTML
func (gnc *GoogleNewsClient) parseGoogleSearchNewsHTML(doc *goquery.Document, query string) []*NewsArticle {
	var articles []*NewsArticle

	doc.Find(".SoaBEf, .WlydOe, .g").Each(func(i int, s *goquery.Selection) {
		article := gnc.extractGoogleSearchNewsArticle(s, query)
		if article != nil {
			articles = append(articles, article)
		}
	})

	return articles
}

// extractGoogleNewsArticle extracts a single article from Google News
func (gnc *GoogleNewsClient) extractGoogleNewsArticle(s *goquery.Selection, query string) *NewsArticle {
	// Extract title
	title := ""
	titleSelectors := []string{"h3", "h4", "[role='heading']", ".JtKRv"}
	for _, sel := range titleSelectors {
		if t := strings.TrimSpace(s.Find(sel).Text()); t != "" {
			title = t
			break
		}
	}

	if title == "" {
		return nil
	}

	// Extract URL
	link := s.Find("a").First()
	href, exists := link.Attr("href")
	if !exists {
		return nil
	}

	articleURL := gnc.cleanGoogleURL(href)

	// Extract source
	source := strings.TrimSpace(s.Find("[data-n-tid], .wEwyrc").Text())
	if source == "" {
		source = "Google News"
	}

	// Extract time
	timeText := strings.TrimSpace(s.Find("time, .WW6dff").Text())
	publishedAt := gnc.parseTimeText(timeText)

	// Extract content/snippet
	content := strings.TrimSpace(s.Find(".st, .Y3v8qd").Text())

	return &NewsArticle{
		Title:       title,
		Content:     content,
		URL:         articleURL,
		Source:      source,
		PublishedAt: publishedAt,
		Keywords:    []string{query},
		Metadata: map[string]string{
			"scraper":      "google_news_enhanced",
			"original_url": href,
			"time_text":    timeText,
		},
	}
}

// extractGoogleSearchNewsArticle extracts a single article from Google Search News
func (gnc *GoogleNewsClient) extractGoogleSearchNewsArticle(s *goquery.Selection, query string) *NewsArticle {
	// Extract title
	title := strings.TrimSpace(s.Find("h3, .LC20lb").Text())
	if title == "" {
		return nil
	}

	// Extract URL
	link := s.Find("a").First()
	href, exists := link.Attr("href")
	if !exists {
		return nil
	}

	articleURL := gnc.cleanGoogleURL(href)

	// Extract source and time
	sourceTimeText := strings.TrimSpace(s.Find(".fG8Fp, .slp").Text())
	source, timeText := gnc.parseSourceTime(sourceTimeText)

	publishedAt := gnc.parseTimeText(timeText)

	// Extract snippet
	content := strings.TrimSpace(s.Find(".st, .s3v9rd").Text())

	return &NewsArticle{
		Title:       title,
		Content:     content,
		URL:         articleURL,
		Source:      source,
		PublishedAt: publishedAt,
		Keywords:    []string{query},
		Metadata: map[string]string{
			"scraper":         "google_search_news",
			"original_url":    href,
			"source_time_raw": sourceTimeText,
		},
	}
}

// cleanGoogleURL cleans Google redirect URLs
func (gnc *GoogleNewsClient) cleanGoogleURL(googleURL string) string {
	// Handle Google News redirects
	if strings.Contains(googleURL, "/url?") {
		parts := strings.Split(googleURL, "url=")
		if len(parts) > 1 {
			decoded, err := url.QueryUnescape(parts[1])
			if err == nil {
				// Remove additional parameters
				if idx := strings.Index(decoded, "&"); idx != -1 {
					decoded = decoded[:idx]
				}
				return decoded
			}
		}
	}

	// Handle relative URLs
	if strings.HasPrefix(googleURL, "./") {
		return "https://news.google.com" + googleURL[1:]
	}

	if strings.HasPrefix(googleURL, "/") {
		return "https://news.google.com" + googleURL
	}

	return googleURL
}

// parseSourceTime parses source and time from combined text
func (gnc *GoogleNewsClient) parseSourceTime(text string) (source, timeText string) {
	// Common patterns: "Source - 3 hours ago", "Source Â· 1 day ago"
	separators := []string{" - ", " Â· ", " â€” ", " | "}

	for _, sep := range separators {
		if parts := strings.Split(text, sep); len(parts) >= 2 {
			source = strings.TrimSpace(parts[0])
			timeText = strings.TrimSpace(parts[len(parts)-1])
			return
		}
	}

	// If no separator found, assume the whole text is source
	source = text
	return
}

// parseTimeText converts time text to actual time
func (gnc *GoogleNewsClient) parseTimeText(timeText string) time.Time {
	now := time.Now()
	timeText = strings.ToLower(strings.TrimSpace(timeText))

	// Handle various time formats
	patterns := map[*regexp.Regexp]func([]string) time.Duration{
		regexp.MustCompile(`(\d+)\s*minutes?\s*ago`): func(matches []string) time.Duration {
			if len(matches) > 1 {
				if mins := parseNumber(matches[1]); mins > 0 {
					return time.Duration(mins) * time.Minute
				}
			}
			return 0
		},
		regexp.MustCompile(`(\d+)\s*hours?\s*ago`): func(matches []string) time.Duration {
			if len(matches) > 1 {
				if hours := parseNumber(matches[1]); hours > 0 {
					return time.Duration(hours) * time.Hour
				}
			}
			return 0
		},
		regexp.MustCompile(`(\d+)\s*days?\s*ago`): func(matches []string) time.Duration {
			if len(matches) > 1 {
				if days := parseNumber(matches[1]); days > 0 {
					return time.Duration(days) * 24 * time.Hour
				}
			}
			return 0
		},
	}

	for pattern, handler := range patterns {
		if matches := pattern.FindStringSubmatch(timeText); len(matches) > 0 {
			if duration := handler(matches); duration > 0 {
				return now.Add(-duration)
			}
		}
	}

	// Default to recent time if can't parse
	return now.Add(-1 * time.Hour)
}

// removeDuplicates removes duplicate articles based on URL and title
func (gnc *GoogleNewsClient) removeDuplicates(articles []*NewsArticle) []*NewsArticle {
	seen := make(map[string]bool)
	var unique []*NewsArticle

	for _, article := range articles {
		key := fmt.Sprintf("%s|%s", article.URL, article.Title)
		if !seen[key] {
			seen[key] = true
			unique = append(unique, article)
		}
	}

	return unique
}

// GetFinanceNews gets finance-related news from Google News
func (gnc *GoogleNewsClient) GetFinanceNews(maxResults int, config *Config) ([]*NewsArticle, error) {
	financeQueries := []string{
		"stock market",
		"financial news",
		"economy",
		"trading",
		"investment",
	}

	var allArticles []*NewsArticle
	articlesPerQuery := maxResults / len(financeQueries)
	if articlesPerQuery < 1 {
		articlesPerQuery = 1
	}

	for _, query := range financeQueries {
		params := EnhancedGoogleNewsParams{
			Query:      query,
			Language:   "en",
			Country:    "US",
			MaxResults: articlesPerQuery,
			SortBy:     "date",
			Category:   "business",
		}

		articles, err := gnc.GetGoogleNews(params, config)
		if err != nil {
			continue // Skip failed queries
		}

		allArticles = append(allArticles, articles...)
	}

	// Remove duplicates and sort by time
	allArticles = gnc.removeDuplicates(allArticles)

	// Sort by published time (newest first)
	for i := 0; i < len(allArticles)-1; i++ {
		for j := i + 1; j < len(allArticles); j++ {
			if allArticles[i].PublishedAt.Before(allArticles[j].PublishedAt) {
				allArticles[i], allArticles[j] = allArticles[j], allArticles[i]
			}
		}
	}

	if len(allArticles) > maxResults {
		allArticles = allArticles[:maxResults]
	}

	return allArticles, nil
}

// GetStockNews gets news for a specific stock symbol
func (gnc *GoogleNewsClient) GetStockNews(symbol string, maxResults int, config *Config) ([]*NewsArticle, error) {
	if strings.TrimSpace(symbol) == "" {
		return nil, fmt.Errorf("stock symbol cannot be empty")
	}

	symbol = strings.ToUpper(strings.TrimSpace(symbol))

	// Try multiple search queries for the stock
	queries := []string{
		fmt.Sprintf("%s stock news", symbol),
		fmt.Sprintf("%s earnings", symbol),
		fmt.Sprintf("%s financial", symbol),
		symbol, // Just the symbol
	}

	var allArticles []*NewsArticle
	articlesPerQuery := maxResults / len(queries)
	if articlesPerQuery < 1 {
		articlesPerQuery = 1
	}

	for _, query := range queries {
		params := EnhancedGoogleNewsParams{
			Query:      query,
			Language:   "en",
			Country:    "US",
			MaxResults: articlesPerQuery,
			SortBy:     "date",
			Category:   "business",
		}

		articles, err := gnc.GetGoogleNews(params, config)
		if err != nil {
			continue
		}

		// Filter articles that actually mention the stock symbol
		var relevantArticles []*NewsArticle
		for _, article := range articles {
			if gnc.containsStockSymbol(article, symbol) {
				relevantArticles = append(relevantArticles, article)
			}
		}

		allArticles = append(allArticles, relevantArticles...)
	}

	// Remove duplicates and limit results
	allArticles = gnc.removeDuplicates(allArticles)
	if len(allArticles) > maxResults {
		allArticles = allArticles[:maxResults]
	}

	return allArticles, nil
}

// containsStockSymbol checks if an article mentions a stock symbol
func (gnc *GoogleNewsClient) containsStockSymbol(article *NewsArticle, symbol string) bool {
	text := strings.ToUpper(article.Title + " " + article.Content)

	// Check for various formats
	patterns := []string{
		fmt.Sprintf(" %s ", symbol),      // " AAPL "
		fmt.Sprintf("(%s)", symbol),      // (AAPL)
		fmt.Sprintf("%s:", symbol),       // AAPL:
		fmt.Sprintf("$%s", symbol),       // $AAPL
	}

	for _, pattern := range patterns {
		if strings.Contains(text, strings.ToUpper(pattern)) {
			return true
		}
	}

	// Use regex for word boundary matching
	regex := regexp.MustCompile(fmt.Sprintf(`\b%s\b`, regexp.QuoteMeta(symbol)))
	return regex.MatchString(text)
}

// parseNumber safely converts string to int
func parseNumber(s string) int {
	var result int
	fmt.Sscanf(s, "%d", &result)
	return result
}

// GetArticleContent è·å–æ–‡ç« çš„å…·ä½“å†…å®¹
func (gnc *GoogleNewsClient) GetArticleContent(articleURL string) (string, error) {
	if strings.TrimSpace(articleURL) == "" {
		return "", fmt.Errorf("article URL cannot be empty")
	}

	// æ£€æŸ¥ç¼“å­˜
	var cached string
	if gnc.cache.Get("article_content", "url", articleURL, &cached) {
		return cached, nil
	}

	// å¦‚æœæ˜¯Google Newsé“¾æ¥ï¼Œå°è¯•è·å–å®é™…ç›®æ ‡URL
	actualURL := articleURL
	if strings.Contains(articleURL, "news.google.com") {
		if redirectURL, err := gnc.followRedirect(articleURL); err == nil && redirectURL != "" {
			actualURL = redirectURL
			fmt.Printf("  è·Ÿéšé‡å®šå‘åˆ°: %s\n", actualURL)
		}
	}

	var content string
	err := WithRetry(DefaultRetryConfig(), func() error {
		resp, err := gnc.client.R().Get(actualURL)
		if err != nil {
			return fmt.Errorf("failed to fetch article: %w", err)
		}

		if resp.StatusCode() != 200 {
			return fmt.Errorf("HTTP error %d when fetching article", resp.StatusCode())
		}

		// è§£æHTMLå†…å®¹
		doc, err := goquery.NewDocumentFromReader(strings.NewReader(resp.String()))
		if err != nil {
			return fmt.Errorf("failed to parse HTML: %w", err)
		}

		content = gnc.extractArticleContent(doc)
		return nil
	})

	if err != nil {
		return "", err
	}

	// ç¼“å­˜ç»“æœ
	gnc.cache.Set("article_content", "url", articleURL, content)

	return content, nil
}

// followRedirect è·ŸéšGoogle Newsé‡å®šå‘è·å–å®é™…URL
func (gnc *GoogleNewsClient) followRedirect(googleURL string) (string, error) {
	// å¯¹äºGoogle Newsçš„readé“¾æ¥ï¼Œå°è¯•è§£æé¡µé¢ä¸­çš„å®é™…é“¾æ¥
	resp, err := gnc.client.R().Get(googleURL)
	if err != nil {
		return "", fmt.Errorf("failed to fetch Google News page: %w", err)
	}

	if resp.StatusCode() != 200 {
		return "", fmt.Errorf("HTTP error %d when fetching Google News page", resp.StatusCode())
	}

	// è§£æHTMLå†…å®¹æŸ¥æ‰¾å®é™…çš„æ–°é—»é“¾æ¥
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(resp.String()))
	if err != nil {
		return "", fmt.Errorf("failed to parse Google News HTML: %w", err)
	}

	// æŸ¥æ‰¾å¯èƒ½çš„åŸå§‹æ–°é—»é“¾æ¥
	var actualURL string

	// å°è¯•ä»å„ç§å¯èƒ½çš„é€‰æ‹©å™¨ä¸­æ‰¾åˆ°åŸå§‹é“¾æ¥
	selectors := []string{
		"a[href*='http']",
		"article a",
		".article a",
		"[data-url]",
	}

	for _, selector := range selectors {
		doc.Find(selector).Each(func(i int, s *goquery.Selection) {
			if actualURL != "" {
				return // å·²ç»æ‰¾åˆ°äº†
			}

			href := s.AttrOr("href", "")
			if href == "" {
				href = s.AttrOr("data-url", "")
			}

			// æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„æ–°é—»ç½‘ç«™URL
			if href != "" && !strings.Contains(href, "google.com") &&
			   (strings.HasPrefix(href, "http://") || strings.HasPrefix(href, "https://")) {
				actualURL = href
			}
		})

		if actualURL != "" {
			break
		}
	}

	if actualURL == "" {
		return googleURL, nil // å¦‚æœæ‰¾ä¸åˆ°ï¼Œè¿”å›åŸURL
	}

	return actualURL, nil
}

// extractArticleContent ä»HTMLä¸­æå–æ–‡ç« å†…å®¹
func (gnc *GoogleNewsClient) extractArticleContent(doc *goquery.Document) string {
	var contentParts []string

	// å°è¯•å¤šç§å†…å®¹é€‰æ‹©å™¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
	contentSelectors := []string{
		// é€šç”¨æ–‡ç« å†…å®¹é€‰æ‹©å™¨
		"article p",
		".article-content p",
		".entry-content p",
		".post-content p",
		".content p",
		".story-body p",
		".article-body p",
		// æ›´é€šç”¨çš„æ®µè½é€‰æ‹©å™¨
		"main p",
		".main p",
		"#content p",
		// æœ€åå°è¯•æ‰€æœ‰æ®µè½
		"p",
	}

	for _, selector := range contentSelectors {
		doc.Find(selector).Each(func(i int, s *goquery.Selection) {
			text := strings.TrimSpace(s.Text())
			if len(text) > 20 && !gnc.isNavigationText(text) {
				contentParts = append(contentParts, text)
			}
		})

		// å¦‚æœæ‰¾åˆ°è¶³å¤Ÿçš„å†…å®¹ï¼Œå°±åœæ­¢
		if len(contentParts) >= 3 {
			break
		}
	}

	// åˆå¹¶å†…å®¹æ®µè½
	fullContent := strings.Join(contentParts, "\n\n")

	// å¦‚æœå†…å®¹å¤ªçŸ­ï¼Œå°è¯•å…¶ä»–æ–¹æ³•
	if len(fullContent) < 50 {
		// å°è¯•è·å–meta description
		if desc := doc.Find("meta[name='description']").AttrOr("content", ""); desc != "" {
			return desc
		}

		// å°è¯•è·å–og:description
		if ogDesc := doc.Find("meta[property='og:description']").AttrOr("content", ""); ogDesc != "" {
			return ogDesc
		}
	}

	return fullContent
}

// isNavigationText åˆ¤æ–­æ˜¯å¦ä¸ºå¯¼èˆªæ–‡æœ¬ï¼ˆéœ€è¦è¿‡æ»¤æ‰ï¼‰
func (gnc *GoogleNewsClient) isNavigationText(text string) bool {
	text = strings.ToLower(text)

	// å¸¸è§çš„å¯¼èˆªå’Œæ— å…³æ–‡æœ¬
	navigationPatterns := []string{
		"subscribe", "sign in", "menu", "search", "home", "about",
		"contact", "privacy", "terms", "cookie", "advertisement",
		"share", "tweet", "facebook", "linkedin", "email",
		"read more", "continue reading", "related articles",
		"you may also like", "recommended", "trending",
	}

	for _, pattern := range navigationPatterns {
		if strings.Contains(text, pattern) {
			return true
		}
	}

	return false
}

// GetNewsWithContent è·å–æ–°é—»å¹¶åŒæ—¶æå–å†…å®¹
func (gnc *GoogleNewsClient) GetNewsWithContent(params EnhancedGoogleNewsParams, config *Config) ([]*NewsArticle, error) {
	// é¦–å…ˆè·å–æ–°é—»åˆ—è¡¨
	articles, err := gnc.GetGoogleNews(params, config)
	if err != nil {
		return nil, err
	}

	// ä¸ºæ¯ç¯‡æ–‡ç« è·å–å†…å®¹ï¼ˆé™åˆ¶æ•°é‡é¿å…è¿‡å¤šè¯·æ±‚ï¼‰
	maxContentArticles := params.MaxResults
	if maxContentArticles > 5 {
		maxContentArticles = 5 // æœ€å¤š5ç¯‡
	}
	if len(articles) < maxContentArticles {
		maxContentArticles = len(articles)
	}

	fmt.Printf("æ­£åœ¨è·å– %d ç¯‡æ–‡ç« çš„å…·ä½“å†…å®¹...\n", maxContentArticles)

	for i := 0; i < maxContentArticles; i++ {
		article := articles[i]

		fmt.Printf("è·å–æ–‡ç« å†…å®¹ %d/%d: %s\n", i+1, maxContentArticles, article.Title)

		content, err := gnc.GetArticleContent(article.URL)
		if err != nil {
			fmt.Printf("  è·å–å†…å®¹å¤±è´¥: %v\n", err)
			continue
		}

		if len(content) > 50 {
			article.Content = content
			fmt.Printf("  æˆåŠŸè·å– %d å­—ç¬¦å†…å®¹\n", len(content))
		} else {
			fmt.Printf("  å†…å®¹å¤ªçŸ­: %d å­—ç¬¦ - %s\n", len(content), content)
		}

		// æ·»åŠ å»¶è¿Ÿé¿å…è¯·æ±‚å¤ªé¢‘ç¹
		time.Sleep(1 * time.Second)
	}

	return articles, nil
}

// GetGoogleNewsRSS é€šè¿‡RSS feedè·å–Google News
func (gnc *GoogleNewsClient) GetGoogleNewsRSS(params EnhancedGoogleNewsParams, config *Config) ([]*NewsArticle, error) {
	rssURL := gnc.buildGoogleNewsRSSURL(params)

	fmt.Printf("ğŸ“¡ æ­£åœ¨é€šè¿‡RSSè·å–Google News: %s\n", params.Query)

	// æ£€æŸ¥ç¼“å­˜
	cacheKey := fmt.Sprintf("rss_%s_%s_%s", params.Query, params.Language, params.Country)
	var cached []*NewsArticle
	if gnc.cache.Get("google_news_rss", "query", cacheKey, &cached) {
		fmt.Printf("âœ… ä»ç¼“å­˜è·å–åˆ° %d ç¯‡RSSæ–‡ç« \n", len(cached))
		return cached, nil
	}

	var articles []*NewsArticle
	err := WithRetry(DefaultRetryConfig(), func() error {
		resp, err := gnc.client.R().Get(rssURL)
		if err != nil {
			return fmt.Errorf("failed to fetch RSS feed: %w", err)
		}

		if resp.StatusCode() != 200 {
			return fmt.Errorf("HTTP error %d when fetching RSS feed", resp.StatusCode())
		}

		// è§£æRSS XML
		var rss RSS
		if err := xml.Unmarshal(resp.Body(), &rss); err != nil {
			return fmt.Errorf("failed to parse RSS XML: %w", err)
		}

		// è½¬æ¢RSSé¡¹ç›®ä¸ºNewsArticle
		for i, item := range rss.Channel.Items {
			if params.MaxResults > 0 && i >= params.MaxResults {
				break
			}

			article := gnc.convertRSSItemToNewsArticle(item, params.Query)
			articles = append(articles, article)
		}

		return nil
	})

	if err != nil {
		return nil, err
	}

	// ç¼“å­˜ç»“æœ
	gnc.cache.Set("google_news_rss", "query", cacheKey, articles)

	// ä¿å­˜åˆ°æ–‡ä»¶
	if config.DataDir != "" {
		gnc.saveRSSArticlesToFile(articles, params, config.DataDir)
	}

	fmt.Printf("âœ… RSSæ¨¡å¼è·å–åˆ° %d ç¯‡æ–‡ç« \n", len(articles))
	return articles, nil
}

// buildGoogleNewsRSSURL æ„å»ºGoogle News RSS URL
func (gnc *GoogleNewsClient) buildGoogleNewsRSSURL(params EnhancedGoogleNewsParams) string {
	baseURL := "https://news.google.com/rss"

	// æ„å»ºæŸ¥è¯¢å‚æ•°
	v := url.Values{}

	// æ·»åŠ æœç´¢æŸ¥è¯¢
	if params.Query != "" {
		if params.Site != "" {
			// å¦‚æœæŒ‡å®šäº†ç½‘ç«™ï¼Œä½¿ç”¨site:è¯­æ³•
			v.Set("q", fmt.Sprintf("%s site:%s", params.Query, params.Site))
		} else {
			v.Set("q", params.Query)
		}
	}

	// æ·»åŠ è¯­è¨€å’Œåœ°åŒº
	if params.Language != "" {
		v.Set("hl", params.Language)
	}
	if params.Country != "" {
		v.Set("gl", params.Country)
		v.Set("ceid", fmt.Sprintf("%s:%s", params.Country, strings.Split(params.Language, "-")[0]))
	}

	// æ„å»ºæœ€ç»ˆURL
	if len(v) > 0 {
		return baseURL + "/search?" + v.Encode()
	}

	// å¦‚æœæ²¡æœ‰æœç´¢å‚æ•°ï¼Œä½¿ç”¨åˆ†ç±»URL
	if params.Category != "" {
		switch params.Category {
		case "business":
			return baseURL + "/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRFZ4ZERBU0FtVnVHZ0pWVXlnQVAB"
		case "technology":
			return baseURL + "/topics/CAAqJggKIiBDQkFTRWdvSUwyMHZNRGx6TVdZU0FtVnVHZ0pWVXlnQVAB"
		case "health":
			return baseURL + "/topics/CAAqJQgKIh9DQkFTRVFvSUwyMHZNR3QwTlRFU0FtVnVHZ0pWVXlnQVAB"
		}
	}

	return baseURL
}

// convertRSSItemToNewsArticle å°†RSSé¡¹ç›®è½¬æ¢ä¸ºNewsArticle
func (gnc *GoogleNewsClient) convertRSSItemToNewsArticle(item Item, query string) *NewsArticle {
	// è§£æå‘å¸ƒæ—¶é—´
	pubTime, err := time.Parse(time.RFC1123Z, item.PubDate)
	if err != nil {
		// å°è¯•å…¶ä»–æ—¶é—´æ ¼å¼
		pubTime, _ = time.Parse("Mon, 02 Jan 2006 15:04:05 MST", item.PubDate)
	}
	if pubTime.IsZero() {
		pubTime = time.Now()
	}

	// æå–æ¥æºä¿¡æ¯
	source := item.Source.Text
	if source == "" && item.Source.URL != "" {
		// ä»URLæå–åŸŸåä½œä¸ºæ¥æº
		if u, err := url.Parse(item.Source.URL); err == nil {
			source = u.Host
		}
	}

	// æ¸…ç†HTMLæ ‡ç­¾ä»descriptionä¸­è·å–çº¯æ–‡æœ¬å†…å®¹
	cleanContent := gnc.cleanHTMLContent(item.Description)

	return &NewsArticle{
		Title:       strings.TrimSpace(item.Title),
		Content:     cleanContent,
		URL:         item.Link,
		Source:      source,
		PublishedAt: pubTime,
		Keywords:    []string{query},
		Metadata: map[string]string{
			"scraper":     "google_news_rss",
			"guid":        item.GUID,
			"source_url":  item.Source.URL,
		},
	}
}

// saveRSSArticlesToFile ä¿å­˜RSSæ–‡ç« åˆ°æ–‡ä»¶
func (gnc *GoogleNewsClient) saveRSSArticlesToFile(articles []*NewsArticle, params EnhancedGoogleNewsParams, dataDir string) {
	newsDir := filepath.Join(dataDir, "news_data")

	// åˆ›å»ºç›®å½•
	if err := os.MkdirAll(newsDir, 0755); err != nil {
		fmt.Printf("æ— æ³•åˆ›å»ºç›®å½• %s: %v\n", newsDir, err)
		return
	}

	// ç”Ÿæˆæ–‡ä»¶å
	today := time.Now().Format("2006-01-02")
	querySlug := strings.ReplaceAll(strings.ToLower(params.Query), " ", "_")
	filename := fmt.Sprintf("google_news_rss_%s_%s.json", querySlug, today)
	filePath := filepath.Join(newsDir, filename)

	// ä¿å­˜æ–‡ç« ä¸ºJSON
	data, err := json.MarshalIndent(articles, "", "  ")
	if err != nil {
		fmt.Printf("æ— æ³•åºåˆ—åŒ–æ–‡ç« æ•°æ®: %v\n", err)
		return
	}

	if err := os.WriteFile(filePath, data, 0644); err != nil {
		fmt.Printf("æ— æ³•å†™å…¥æ–‡ä»¶ %s: %v\n", filePath, err)
		return
	}

	fmt.Printf("ğŸ“ RSSæ–‡ç« å·²ä¿å­˜åˆ°: %s\n", filePath)
}

// cleanHTMLContent æ¸…ç†HTMLæ ‡ç­¾å¹¶æå–çº¯æ–‡æœ¬å†…å®¹
func (gnc *GoogleNewsClient) cleanHTMLContent(htmlContent string) string {
	if htmlContent == "" {
		return ""
	}

	// ä½¿ç”¨goqueryè§£æHTMLå¹¶æå–æ–‡æœ¬
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(htmlContent))
	if err != nil {
		// å¦‚æœè§£æå¤±è´¥ï¼Œä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç®€å•å»é™¤HTMLæ ‡ç­¾
		return gnc.stripHTMLTags(htmlContent)
	}

	// æå–æ‰€æœ‰æ–‡æœ¬å†…å®¹
	text := strings.TrimSpace(doc.Text())

	// å¦‚æœæå–çš„æ–‡æœ¬ä¸ºç©ºï¼Œå°è¯•æ­£åˆ™è¡¨è¾¾å¼æ–¹æ³•
	if text == "" {
		return gnc.stripHTMLTags(htmlContent)
	}

	return text
}

// stripHTMLTags ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å»é™¤HTMLæ ‡ç­¾
func (gnc *GoogleNewsClient) stripHTMLTags(content string) string {
	// å»é™¤HTMLæ ‡ç­¾
	htmlTagRegex := regexp.MustCompile(`<[^>]*>`)
	content = htmlTagRegex.ReplaceAllString(content, "")

	// è§£ç HTMLå®ä½“
	content = strings.ReplaceAll(content, "&nbsp;", " ")
	content = strings.ReplaceAll(content, "&amp;", "&")
	content = strings.ReplaceAll(content, "&lt;", "<")
	content = strings.ReplaceAll(content, "&gt;", ">")
	content = strings.ReplaceAll(content, "&quot;", "\"")
	content = strings.ReplaceAll(content, "&#39;", "'")

	// æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
	spaceRegex := regexp.MustCompile(`\s+`)
	content = spaceRegex.ReplaceAllString(content, " ")

	return strings.TrimSpace(content)
}

// GetDirectNewsRSS ç›´æ¥ä»å„å¤§æ–°é—»æºRSSè·å–å¸¦çœŸå®æ‘˜è¦çš„æ–°é—»
func (gnc *GoogleNewsClient) GetDirectNewsRSS(query string, maxResults int, config *Config) ([]*NewsArticle, error) {
	fmt.Printf("ğŸ“¡ æ­£åœ¨ä»å¤šä¸ªç›´æ¥æ–°é—»æºè·å–: %s\n", query)

	// ä¸»è¦æ–°é—»æºRSSåˆ—è¡¨
	rssFeeds := []struct {
		Name string
		URL  string
	}{
		{"BBC News", "https://feeds.bbci.co.uk/news/rss.xml"},
		{"Reuters", "https://www.reuters.com/tools/rss"},
		{"CNN", "http://rss.cnn.com/rss/edition.rss"},
		{"TechCrunch", "https://techcrunch.com/feed/"},
		{"Yahoo Finance", "https://finance.yahoo.com/rss/"},
	}

	var allArticles []*NewsArticle

	for _, feed := range rssFeeds {
		fmt.Printf("  æ­£åœ¨è·å– %s...\n", feed.Name)

		articles, err := gnc.fetchDirectRSSFeed(feed.URL, feed.Name, query, maxResults/len(rssFeeds))
		if err != nil {
			fmt.Printf("  âš ï¸ %s è·å–å¤±è´¥: %v\n", feed.Name, err)
			continue
		}

		allArticles = append(allArticles, articles...)
		if len(allArticles) >= maxResults {
			break
		}
	}

	// é™åˆ¶ç»“æœæ•°é‡
	if len(allArticles) > maxResults {
		allArticles = allArticles[:maxResults]
	}

	fmt.Printf("âœ… ä»ç›´æ¥RSSæºè·å–åˆ° %d ç¯‡æ–‡ç« \n", len(allArticles))
	return allArticles, nil
}

// fetchDirectRSSFeed ä»å•ä¸ªRSSæºè·å–æ–°é—»
func (gnc *GoogleNewsClient) fetchDirectRSSFeed(rssURL, sourceName, query string, maxResults int) ([]*NewsArticle, error) {
	var articles []*NewsArticle

	err := WithRetry(DefaultRetryConfig(), func() error {
		resp, err := gnc.client.R().Get(rssURL)
		if err != nil {
			return fmt.Errorf("failed to fetch RSS feed: %w", err)
		}

		if resp.StatusCode() != 200 {
			return fmt.Errorf("HTTP error %d", resp.StatusCode())
		}

		// è§£æRSS XML
		var rss RSS
		if err := xml.Unmarshal(resp.Body(), &rss); err != nil {
			return fmt.Errorf("failed to parse RSS XML: %w", err)
		}

		// è¿‡æ»¤ä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡ç« 
		count := 0
		for _, item := range rss.Channel.Items {
			if count >= maxResults {
				break
			}

			// ç®€å•çš„å…³é”®è¯åŒ¹é…
			if query != "" && !gnc.containsKeyword(item.Title+item.Description, query) {
				continue
			}

			article := gnc.convertDirectRSSItem(item, sourceName, query)
			articles = append(articles, article)
			count++
		}

		return nil
	})

	return articles, err
}

// containsKeyword æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«å…³é”®è¯
func (gnc *GoogleNewsClient) containsKeyword(text, keyword string) bool {
	text = strings.ToLower(text)
	keyword = strings.ToLower(keyword)

	// æ”¯æŒå¤šä¸ªå…³é”®è¯
	keywords := strings.Fields(keyword)
	for _, kw := range keywords {
		if strings.Contains(text, kw) {
			return true
		}
	}
	return false
}

// convertDirectRSSItem è½¬æ¢ç›´æ¥RSSé¡¹ç›®ä¸ºNewsArticle
func (gnc *GoogleNewsClient) convertDirectRSSItem(item Item, sourceName, query string) *NewsArticle {
	// è§£æå‘å¸ƒæ—¶é—´
	pubTime, _ := time.Parse(time.RFC1123Z, item.PubDate)
	if pubTime.IsZero() {
		pubTime, _ = time.Parse("Mon, 02 Jan 2006 15:04:05 MST", item.PubDate)
	}
	if pubTime.IsZero() {
		pubTime = time.Now()
	}

	// æ¸…ç†æè¿°å†…å®¹
	cleanDescription := gnc.cleanHTMLContent(item.Description)

	return &NewsArticle{
		Title:       strings.TrimSpace(item.Title),
		Content:     cleanDescription, // è¿™é‡Œæ˜¯çœŸæ­£çš„æ‘˜è¦å†…å®¹
		URL:         item.Link,
		Source:      sourceName,
		PublishedAt: pubTime,
		Keywords:    []string{query},
		Metadata: map[string]string{
			"scraper":    "direct_rss",
			"guid":       item.GUID,
			"rss_source": sourceName,
		},
	}
}